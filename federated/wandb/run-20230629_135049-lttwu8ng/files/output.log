










epoch: 10, loss: 0.72, time: 64.16:   2%|█▍                                                                | 9/400 [11:33<7:22:06, 67.84s/it]











epoch: 20, loss: 0.61, time: 67.38:   5%|███                                                              | 19/400 [23:25<7:07:11, 67.28s/it]











epoch: 30, loss: 0.51, time: 65.38:   7%|████▋                                                            | 29/400 [34:57<6:45:50, 65.63s/it]











epoch: 40, loss: 0.48, time: 64.69:  10%|██████▎                                                          | 39/400 [46:36<6:43:17, 67.03s/it]











epoch: 50, loss: 0.49, time: 68.40:  12%|███████▉                                                         | 49/400 [58:23<6:34:34, 67.45s/it]











epoch: 60, loss: 0.48, time: 66.90:  15%|█████████▎                                                     | 59/400 [1:10:06<6:23:01, 67.40s/it]











epoch: 70, loss: 0.48, time: 63.59:  17%|██████████▊                                                    | 69/400 [1:21:49<6:13:05, 67.63s/it]











epoch: 80, loss: 0.48, time: 66.03:  20%|████████████▍                                                  | 79/400 [1:33:39<5:59:40, 67.23s/it]












epoch: 90, loss: 0.48, time: 67.48:  22%|██████████████▏                                                | 90/400 [1:46:15<6:48:51, 79.14s/it]











epoch: 100, loss: 0.47, time: 65.26:  25%|███████████████▎                                             | 100/400 [1:58:02<6:34:16, 78.85s/it]











epoch: 110, loss: 0.48, time: 64.75:  28%|████████████████▊                                            | 110/400 [2:09:55<6:19:15, 78.47s/it]










epoch: 120, loss: 0.48, time: 67.48:  30%|██████████████████▏                                          | 119/400 [2:21:08<5:16:34, 67.59s/it]












epoch: 130, loss: 0.48, time: 66.50:  32%|███████████████████▊                                         | 130/400 [2:33:43<5:58:04, 79.57s/it]










epoch: 140, loss: 0.48, time: 66.87:  35%|█████████████████████▏                                       | 139/400 [2:44:53<4:53:15, 67.42s/it]



epoch: 142, loss: 0.48, time: 66.57:  36%|█████████████████████▋                                       | 142/400 [2:47:53<5:05:03, 70.94s/it]
Traceback (most recent call last):
  File "src/main.py", line 93, in <module>
    main()
  File "src/main.py", line 74, in main
    trained_model = run_server(dataset, num_clients=args.c, epochs=args.epochs,
  File "/home/abenzaamia/NeuCF/federated/src/server.py", line 39, in run_server
    trained_weights = training_process(server_model, clients, num_clients, epochs, local_epochs, dataset, args)
  File "/home/abenzaamia/NeuCF/federated/src/fedmlp/train.py", line 66, in training_process
    w, loss = single_train_round(server_model, clients, local_epochs)
  File "/home/abenzaamia/NeuCF/federated/src/fedmlp/train.py", line 112, in single_train_round
    weights, loss = client.train(server_model_copy, local_epochs)
  File "/home/abenzaamia/NeuCF/federated/src/client.py", line 51, in train
    optimizer.step()
  File "/home/abenzaamia/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "/home/abenzaamia/.venv/lib/python3.8/site-packages/torch/optim/optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/abenzaamia/.venv/lib/python3.8/site-packages/torch/optim/adamw.py", line 171, in step
    adamw(
  File "/home/abenzaamia/.venv/lib/python3.8/site-packages/torch/optim/adamw.py", line 321, in adamw
    func(
  File "/home/abenzaamia/.venv/lib/python3.8/site-packages/torch/optim/adamw.py", line 440, in _single_tensor_adamw
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
KeyboardInterrupt